# -*- coding: utf-8 -*-
"""doc_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oNn7Otn77NlBjJaWFSw7Z7D6E_vmyZkU
"""

from google.colab import drive
drive.mount('/gdrive', force_remount=True)

!pip install ratsnlp

import torch
from ratsnlp.nlpbook.classification import ClassificationTrainArguments
args = ClassificationTrainArguments(
    pretrained_model_name="beomi/kcbert-base",
    downstream_corpus_name="nsmc",
    downstream_model_dir="/gdrive/My Drive/nlpbook/checkpoint-doccls",
    batch_size=32 if torch.cuda.is_available() else 4,
    learning_rate=5e-5,
    max_seq_length=128, # 토큰 기준 입력 문장 최대 길이. 길이 부족한 경우 [PAD]를 붙여준다.
    epochs=3,
    seed=8888,
    tpu_cores=0 if torch.cuda.is_available() else 8
)

# 랜덤 시드 고정
from ratsnlp import nlpbook
nlpbook.set_seed(args)
nlpbook.set_logger(args) # 각종 로그들을 출력하는 로거 설정

# 말뭉치 내려받기
from Korpora import Korpora
Korpora.fetch(
    corpus_name=args.downstream_corpus_name,
    root_dir=args.downstream_corpus_root_dir,
    force_download=True
)

# 토크나이저 준비
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained(
    args.pretrained_model_name,
    do_lower_case=False # True면 모두 소문자로 변환,  False는 대/소문자 구분
)

# 데이터셋 구축
from ratsnlp.nlpbook.classification import NsmcCorpus, ClassificationDataset
corpus = NsmcCorpus()
train_dataset = ClassificationDataset(
    args=args,
    corpus=corpus,
    tokenizer=tokenizer,
    mode="train"
)

"""1.    Nscmcorpus는 CSV 파일 형식의 NSMC 데이터를 문장과 레이블(긍정/부정)로 읽어와 ClassificationDataset에 넘겨준다.
2.   ClassificationDataset에서는 모델이 학습할 수 있는 형태로 가공한다
3.  input_ids: 인덱스로 변환된 토큰 시퀀스, attention_mask: 해당 토큰이 패딩(0)토큰인지 아닌지(1) 나타낸다.
4.  token_type_ids: 세그먼트 정보, label: 정수로 바뀐 레이블 정보
"""

print(train_dataset[0])

# 학습용 데이터 로더 구축
from torch.utils.data import DataLoader, RandomSampler
train_dataloader = DataLoader(
    train_dataset,
    batch_size=args.batch_size,
    sampler=RandomSampler(train_dataset, replacement=False),
    collate_fn=nlpbook.data_collator, # 랜덤추출된 인스턴스를 배치로 만드는 역할을 하는 함수
    drop_last=False,
    num_workers=args.cpu_workers
)

# 평가용 데이터 로더 구축
from torch.utils.data import SequentialSampler
val_dataset = ClassificationDataset(
    args=args,
    corpus=corpus,
    tokenizer=tokenizer,
    mode="test"
)

val_dataloader = DataLoader(
    val_dataset,
    batch_size=args.batch_size,
    sampler=SequentialSampler(val_dataset),
    collate_fn=nlpbook.data_collator,
    drop_last=False,
    num_workers=args.cpu_workers
)

# 모델 불러오가(초기화)
from transformers import BertConfig, BertForSequenceClassification
pretrained_model_config = BertConfig.from_pretrained(
    args.pretrained_model_name,
    num_labels=corpus.num_labels
)
model = BertForSequenceClassification.from_pretrained(
    args.pretrained_model_name,
    config=pretrained_model_config
)

# 모델 학습시키기
from ratsnlp.nlpbook.classification import ClassificationTask
task = ClassificationTask(model, args) # defining model, optimizer, training process
trainer = nlpbook.get_trainer(args)
trainer.fit(
    task, 
    train_dataloaders=train_dataloader, 
    val_dataloaders=val_dataloader
    )